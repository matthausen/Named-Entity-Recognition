{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re\n",
    "import heapq\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beautifulsoup4 and lxml are required libraries\n",
    "pip install beautifulsoup4\n",
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')  \n",
    "article = scraped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:  \n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in wikipedia all paragraph are delimited by <p> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Square Brackets and Extra Spaces\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "article_text = re.sub(r'\\s+', ' ', article_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters and digits\n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = nltk.sent_tokenize(article_text)  \n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}  \n",
    "for word in nltk.word_tokenize(formatted_article_text):  \n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the tokenized word is present in the english nltk corpus.\n",
    "If not, checks if it has been encountered (set it's key value to 1)\n",
    "If encountered already, add +1 to its key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequency = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():  \n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}  \n",
    "for sent in sentence_list:  \n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans. Colloquially, the term \"artificial intelligence\" is used to describe machines/computers that mimic \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\". Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning (\"fire together, wire together\"), GMDH or competitive learning. Deep learning has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others. Artificial intelligence can be classified into three different types of systems: analytical, human-inspired, and humanized artificial intelligence. IBM has created its own artificial intelligence computer, the IBM Watson, which has beaten human intelligence (at some levels). Many of the problems in this article may also require general intelligence, if machines are to solve the problems as well as people do.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)  \n",
    "print(summary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
